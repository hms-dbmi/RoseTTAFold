{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BMI702Assgn1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linzi-yu/RoseTTAFold/blob/main/BMI702Assgn1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authored by Yasha Ektefaie and Mert Eden"
      ],
      "metadata": {
        "id": "XgblYbx6kSBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not change the runtime environment from GPU to standard runtime. You'll need GPU for Q3. **"
      ],
      "metadata": {
        "id": "ayXK6Hstkqq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please make a copy of this Colab notebook so that you can save your codes!***"
      ],
      "metadata": {
        "id": "pu9y-NOLkvLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fairness:\n",
        "\n",
        "Code blocks titled 2.1 and 2.2 pertain to questions 2.1 and 2.2 respectively on the homework. Below are some blocks to set you up for training the logistic regression model. It will download the dataset and do some preproccessing. For this exercise *you will not be training the model yourself*. "
      ],
      "metadata": {
        "id": "uHzMd2j1kzW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi-y4FPzkV8E"
      },
      "outputs": [],
      "source": [
        "!wget -O credit.csv https://www.openml.org/data/get_csv/31/dataset_31_credit-g.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "RIV5ZpXykgzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('credit.csv')"
      ],
      "metadata": {
        "id": "AwpJlTKUkkVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove = ['installment_commitment', 'other_parties','housing', 'property_magnitude',\n",
        "          'other_payment_plans','existing_credits', 'residence_since']\n",
        "  \n",
        "cols_keep = [i for i in df.columns if i not in remove]"
      ],
      "metadata": {
        "id": "xKdFfPpUkmUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking status different levels of credit\n",
        "checking_status_mappings = {\"'no checking'\": 0, \"'<0'\": 1, \"'0<=X<200'\":2, \"'>=200'\":3}\n",
        "\n",
        "#credit problem: have they paid (0) if not paid then problem 1\n",
        "credit_status_mappings = {\"'no credits/all paid'\":0, \"'all paid'\":0,\n",
        "                          \"'existing paid'\":0,\n",
        "                          \"'critical/other existing credit'\":1,\n",
        "                          \"'delayed previously'\":1 }\n",
        "\n",
        "#credit pupose: car is 0, education is 1, business is 2, objects: 3, other: 4\n",
        "credit_purpose_mapping = {\n",
        "    \"radio/tv\": 3,\n",
        "    \"education\": 1,\n",
        "    \"furniture/equipment\":3,\n",
        "    \"'new car'\": 0 ,\n",
        "    \"'used car'\":0, \n",
        "    \"business\": 2, \n",
        "    \"'domestic appliance'\": 3,\n",
        "    \"repairs\": 3,\n",
        "    \"other\": 4, \n",
        "    \"retraining\": 4\n",
        "    }\n",
        "\n",
        "#employment: unemployed is 0, <1 is 1, 1<=X<4 is 2, 4<=X<7 is 3, >= 7 is 4\n",
        "employment_mapping = {\n",
        "    \"'>=7'\": 4, \n",
        "    \"'1<=X<4'\": 2, \n",
        "    \"'4<=X<7'\": 3, \n",
        "    'unemployed': 0 , \n",
        "    \"'<1'\": 1\n",
        "}\n",
        "\n",
        "#saving status: no savings is 0, <100 is 1, 100-500 is 2, 500-1000 is 3, >1000 4\n",
        "savings_status = {\"'no known savings'\":0, \n",
        "\"'<100'\":1, \n",
        "\"'500<=X<1000'\":3, \n",
        "\"'>=1000'\":4,\n",
        "\"'100<=X<500'\": 2}\n",
        "\n",
        "#job binning just provide number to each option\n",
        "job_mapping = {\n",
        "    'skilled': 0, \n",
        "    \"'unskilled resident'\": 1, \n",
        "    \"'high qualif/self emp/mgmt'\": 2,\n",
        "    \"'unemp/unskilled non res'\": 3}"
      ],
      "metadata": {
        "id": "_Ugnz8-Hr2lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trunc_df = df[cols_keep].copy()\n",
        "trunc_df = trunc_df.replace({\n",
        "                  \"checking_status\": checking_status_mappings,\n",
        "                  \"purpose\": credit_purpose_mapping, \n",
        "                  \"credit_history\": credit_status_mappings,\n",
        "                  \"employment\": employment_mapping,\n",
        "                  \"job\": job_mapping,\n",
        "                  \"savings_status\":savings_status})\n",
        "\n",
        "#Gender 1 if female, 0 if male\n",
        "trunc_df['sex'] = np.where(trunc_df['personal_status'].str.contains('female'), 1, 0)\n",
        "#Telephone 1 if have telephone, 0 otherwise\n",
        "trunc_df['own_telephone'] = np.where(trunc_df['own_telephone'].str.contains('none'), 0, 1)\n",
        "#Foreign worker 1 if foreign worker, 0 otherwise\n",
        "trunc_df['foreign_worker'] = np.where(trunc_df['foreign_worker'].str.contains('yes'), 1, 0)\n",
        "#Class 1 if good, 0 if bad\n",
        "trunc_df['class'] = np.where(trunc_df['class'].str.contains('good'), 1, 0)\n",
        "del trunc_df['personal_status']\n",
        "trunc_df.head()"
      ],
      "metadata": {
        "id": "w0wrHBErse3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, x_valid, y_train, y_valid = train_test_split(trunc_df.drop('class',axis=1), \n",
        "                                                    trunc_df['class'], test_size=0.30, \n",
        "                                                    random_state=101)\n"
      ],
      "metadata": {
        "id": "jrUauKtasvVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biased_lr = LogisticRegression(max_iter = 1000)\n",
        "biased_lr.fit(X_train,y_train)\n",
        "print(f\"Logistic regression valid accuracy: {biased_lr.score(x_valid, y_valid)}\")"
      ],
      "metadata": {
        "id": "ALdt2Pw0ZSm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Compute the demographic parity with respect to the sensitive trait `sex`.\n",
        "\n",
        "Define a function parity that takes the predictions on the test set of our\n",
        "linear regressor, and a column of sensitive attributes and computes the difference in demographic parity. \n",
        "\n",
        "Run parity on the sensitive attribute `sex` and briefly discuss whether or not this linear regressor satisfies demographic parity."
      ],
      "metadata": {
        "id": "65q7KQa-bBU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we are comparing the rates of good credit among males versus females\n",
        "\n",
        "def parity(pred, sens):\n",
        "  return 0\n",
        "\n",
        "sens = x_valid['sex']\n",
        "pred = biased_lr.predict(x_valid)\n",
        "\n",
        "parity(pred, sens)"
      ],
      "metadata": {
        "id": "7zytyBIWaDhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Compute equal opportunity with respect to the sensitive trait `sex`.\n",
        "Define a function equal_opportunity that takes the predictions on the test set of our linear regressor, a column of ground truth labels, and a column of sensitive attributes and computes the difference in equal opportunity.\n",
        "\n",
        "Compute equal opportunity with respect to the sensitive trait `sex`. How can we interpret the calculated rate of equal opportunity? What is the meaning of this rate for someone looking to get a loan from the bank?"
      ],
      "metadata": {
        "id": "XLbPyYxUlooA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equal_opportunity(pred, labels, sens):\n",
        "  return 0\n",
        "sens = x_valid['sex']\n",
        "labels = y_valid\n",
        "pred = biased_lr.predict(x_valid)\n",
        "equal_opportunity(pred, labels, sens)"
      ],
      "metadata": {
        "id": "aqcrEwJwfMzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Explainibility.\n",
        "\n",
        "**Before you start working on Q3, in case you have changed your runtime, check if you still have GPU resource by clicking on Runtime > Change runtime type. \"Hardware acceleartor\" should be set as \"GPU\". **\n",
        "\n",
        "For the next problem you will be running integrated gradients on a binary classifier trained on the PCam dataset. PCam or PatchCamelyon is a histopathological dataset of lymph nodes. The classifier is trained to detect whether or not there is any metastatis present in these lymph nodes. Below is some starter code to download required packages, install the dataset and train the model. Code blocks are titled with the respective excerise number in the assignment."
      ],
      "metadata": {
        "id": "96Hf2CQcl6tW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up\n",
        "We'll be using pyTorch to train our CNN. **All the NN training is already implemented so you do not have to understand how it works.** Please just change the runtime type as explained above and run the following lines of code one at a time. "
      ],
      "metadata": {
        "id": "Ha2AmUrPl9V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting pytorch\n",
        "!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install captum\n",
        "!pip install flask_compress\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "sHljojTGnsxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1Ka0XfEMiwgCYPdTI-vv6eUElOBnKFKQ2\n",
        "!gdown --id 1269yhu3pZDP8UYFQs-NYs3FPwuK-nGSG"
      ],
      "metadata": {
        "id": "0artcDfGmPKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d camelyonpatch_level_2_split_train_x.h5.gz\n",
        "!gzip -d /content/camelyonpatch_level_2_split_train_y.h5.gz"
      ],
      "metadata": {
        "id": "VYk_OyfB0YPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "jH4OPAOFnjFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/gdrive/My\\ Drive/BMI702/ # You can change directory under 'My Drive' as you want\n",
        "!cp camelyonpatch_level_2_split_train_x.h5 /content/gdrive/My\\ Drive/BMI702/"
      ],
      "metadata": {
        "id": "jUjCYDct0Gfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to Setup Dataset"
      ],
      "metadata": {
        "id": "QpfRTsYJJ8r1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PER_TRAIN = 50 / 100\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch import optim\n",
        "from torch.autograd import Variable \n",
        "from torch.utils import data\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "import h5py \n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "class PCam(torch.utils.data.Dataset):\n",
        "  def __init__(self, transform=None):\n",
        "      super(PCam, self).__init__()\n",
        "      self.images = h5py.File('/content/camelyonpatch_level_2_split_train_x.h5', 'r')\n",
        "      self.labels = h5py.File('/content/camelyonpatch_level_2_split_train_y.h5', 'r')\n",
        "      self.transform=transform\n",
        "\n",
        "  def __getitem__(self, index): \n",
        "    image = torch.tensor(self.images['x'][index]).float()\n",
        "    image = (image / 255.0).permute(2,1,0)\n",
        "    if self.transform:\n",
        "      image = self.transform(image, self.labels['y'][index])\n",
        "\n",
        "    return (image, Variable(torch.tensor(self.labels['y'][index]).reshape(-1).float()))\n",
        "  def __len__(self):\n",
        "    return len(self.images['x'])\n",
        "\n"
      ],
      "metadata": {
        "id": "kQ66c4EU0NCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_transform(tensor, label):\n",
        "  transform = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.ToPILImage(), # Convert np array to PILImage\n",
        "      \n",
        "      # Resize image to 224 x 224 as required by most vision models\n",
        "      torchvision.transforms.Resize(\n",
        "          size=(224, 224)\n",
        "      ),\n",
        "      \n",
        "      # Convert PIL image to tensor with image values in [0, 1]\n",
        "      torchvision.transforms.ToTensor(),\n",
        "  ])\n",
        "  return transform(tensor)\n",
        "\n",
        "\n",
        "pcam = PCam(transform=initial_transform)\n",
        "\n",
        "sz = len(pcam)\n",
        "size_train = math.floor(sz - (sz * (1 - PER_TRAIN)))\n",
        "size_test = sz - size_train\n",
        "\n",
        "print(f\"Training size: {size_train}\")\n",
        "print(f\"Testing size: {size_test}\")\n",
        "\n",
        "\n",
        "train_split, test_split = torch.utils.data.random_split(pcam, [size_train, size_test])\n",
        "train_set = DataLoader(train_split, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, num_workers=1)\n",
        "test_set = DataLoader(test_split, batch_size=1,\n",
        "                        shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "id": "NdLIOoQ02CEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take a peek of our data\n",
        "Our transformed PCAM dataset consists of 262,144 data each consists of a 224*224 image with 3 color channels and 1 label to annotate if the image contains metastatic tissue (1) or not (0)."
      ],
      "metadata": {
        "id": "484B98Gx2JsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = pcam[0]\n",
        "print(\"Image of shape \", image.shape)\n",
        "print(image)\n",
        "print(\"Label: \", label)\n",
        "\n",
        "\"\"\"\n",
        "If everything ran correctly should see:Image of shape  torch.Size([3, 224, 224])\n",
        "And then should see a large printed out 2D Array with text like:\n",
        "\n",
        "tensor([[[0.8863, 0.8863, 0.8784,  ..., 0.3451, 0.3373, 0.3333],\n",
        "\n",
        "printed out.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DkRf4Ywx2Fre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "*********************\n",
        "*********************\n",
        "!!!!!PLEASE READ!!!!!\n",
        "*********************\n",
        "*********************\n",
        "\n",
        "This is the code to train the model. \n",
        "If everything above ran correctly upon running this block you should see\n",
        "lines like: \n",
        "\n",
        "[1,     1] loss: 0.00021\n",
        "Accuracy: 0.010625000111758709\n",
        "\n",
        "Make sure you switched the runtime to GPU as described in the beginning.\n",
        "Even with the GPU this block of code will take around 15-30 minutes to run. \n",
        "As long as lines like the one shown above are being printed and the accuracy\n",
        "is increasing everything is fine. Just don't loose internet connect or let your\n",
        "computer fall asleep during training. \n",
        "\n",
        "It is important you run this block of code once. Do not rerun it as you will\n",
        "rerun training. This is why after this block of code there is code to SAVE\n",
        "your model and LOAD the model. That way you can run training and save the model\n",
        "once and then load as needed.\n",
        "\n",
        "The way all this works is we explicitly create a folder in your google drive \n",
        "where we are storing the training data and saved model. Once the training data\n",
        "and model is saved, even if your runtime disconnects or you close this tab\n",
        "the model and data is still saved to your google drive. All you have to do \n",
        "to start up again is remount to that directory and load in the model. \n",
        "\n",
        "Run these next cells carefully.\n",
        "\n",
        "\"\"\"\n",
        "import torchvision.models as models\n",
        "from tqdm.notebook import tqdm\n",
        "import gc \n",
        "import copy \n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "MY_MODEL = models.resnet18() \n",
        "\n",
        "\"\"\"\n",
        "If you find training is taking too long consider changing epoch here from 2 to 1\n",
        "So instead of epochs = 2, have it be epochs = 1\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def train(dataset, epochs=2):\n",
        "  model = copy.deepcopy(MY_MODEL)\n",
        "\n",
        "  model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 1))\n",
        "\n",
        "  error = nn.BCEWithLogitsLoss()\n",
        "  learning_rate = 0.001\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "  running_loss = 0.0\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model.train()\n",
        "  model = model.to(device)\n",
        "  for epoch in range(epochs):\n",
        "    correct = 0\n",
        "    for i, (train, labels) in enumerate(dataset):\n",
        "            # Clear gradients\n",
        "            train = train.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            if i == 0:\n",
        "              print(\"here\")\n",
        "            # Forward propagation\n",
        "            outputs = model(train)        \n",
        "            # Compute Loss\n",
        "            loss = error(outputs, labels.view(-1, 1).float())\n",
        "            correct += labels.eq(torch.round(torch.sigmoid(outputs))).sum()\n",
        "            # Calculating gradients\n",
        "            loss.backward()        \n",
        "            # Update parameters\n",
        "            optimizer.step()   \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 50 == 0:    # print every 50 mini-batches\n",
        "                print('[%d, %5d] loss: %.5f' %\n",
        "                      (epoch + 1, i + 1, running_loss / (BATCH_SIZE * 50)))\n",
        "                running_loss = 0.0\n",
        "                print(f\"Accuracy: {correct / (BATCH_SIZE * 50)}\")\n",
        "                correct = 0\n",
        "  return model\n",
        "\n",
        "#model = train(train_set)"
      ],
      "metadata": {
        "id": "mXk_zn9G2MfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the code to train the model\n",
        "model = train(train_set)"
      ],
      "metadata": {
        "id": "uzpPsgeuRwyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can save the model to your drive and load it in the future."
      ],
      "metadata": {
        "id": "CZLLABipBW3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/gdrive/My\\ Drive/BMI702/ # You can change the directory under \"My Drive\" as you want."
      ],
      "metadata": {
        "id": "rVQOG_ZQ5JSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to save your model\n",
        "model_save_name = 'pcam_model'\n",
        "path = f\"/content/gdrive/My Drive/BMI702/{model_save_name}\" \n",
        "torch.save(model, path)"
      ],
      "metadata": {
        "id": "9lxcQfsqBZ5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to load the model\n",
        "#If your collab notebook instance crashes you can reload the model by running these lines\n",
        "#We first remount to the google drive folder were everything is stored\n",
        "#Then we just pull from the model file stored in that folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_save_name = 'pcam_model'\n",
        "path = f\"/content/gdrive/My Drive/BMI702/{model_save_name}\" \n",
        "model = torch.load(path)"
      ],
      "metadata": {
        "id": "ihcfSWZHBc-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate how our model did on the first 2000 testing points"
      ],
      "metadata": {
        "id": "YF379bU_BgZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for i, (train, labels) in enumerate(test_set):\n",
        "      if i > 2000:\n",
        "        break\n",
        "      images = train.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(images)\n",
        "      correct += labels.view(-1,1).eq(torch.round(torch.sigmoid(outputs))).sum()\n",
        "      total += 1\n",
        "      \n",
        "\n",
        "print('Accuracy of the network on the 2000 test images: %d %%' % (100 * (correct / total)))"
      ],
      "metadata": {
        "id": "5RP8FTSvBfOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's visualize some slides.\n",
        "\n",
        "Although we're no pathologists, we do get some valuable insight as to what our data is by looking at it. Below is some simple matplotlib code to get you started."
      ],
      "metadata": {
        "id": "s7Cf2uM2Cacl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "from captum.attr import GradientShap\n",
        "from captum.attr import Occlusion\n",
        "from captum.attr import NoiseTunnel\n",
        "from captum.attr import visualization as viz\n",
        "from captum.insights import AttributionVisualizer, Batch\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-kl40r4JCK_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(5,2, figsize=(20,20))\n",
        "\n",
        "for i, (img, label) in enumerate(test_set):\n",
        "  if i >= 10:\n",
        "    break\n",
        "  else:\n",
        "    img = img.squeeze(dim=0)\n",
        "    ax[i//2,i%2].title.set_text(\"Metastases\" if label == 1 else \"Benign\")\n",
        "    ax[i//2,i%2].imshow(img.permute(1,2,0))"
      ],
      "metadata": {
        "id": "14vepZ40bMva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Great! Now you have trained and saved a model. Now we can begin answering some questions! This is where you will have to do some coding."
      ],
      "metadata": {
        "id": "E3j-UVj3Cw_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 How explainable are our results? \n",
        "Run the code below to pick an image from 'test_set' and run integrated gradients. Use the output to explain whether or not the model is really looking for characteristics of metastasis. Report in your results whether or not the integrated gradients tell a meaningful story about the image. We do not expect a medical answer here, any form of reasoning is fine."
      ],
      "metadata": {
        "id": "N7ei8hRvChqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If you run this and get a GPU out of memory error. Restart the kernal and \n",
        "reload the data and reload your saved model. The training caused the GPU\n",
        "memory to get filled up.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def visualize_ig(model, img, label, baseline):\n",
        "  img = img.unsqueeze(0).to(device)\n",
        "  outputs = model(img)\n",
        "  print(f\"Raw model Output {outputs.item()}\")\n",
        "  outputs = nn.functional.softmax(outputs).reshape(-1)\n",
        "  print(f\"Model output after running through softmax {outputs.item()}\")\n",
        "  integrated_gradients = IntegratedGradients(model)\n",
        "\n",
        "  attributions_ig = integrated_gradients.attribute(img, target=0)\n",
        "\n",
        "  default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
        "                                                  [(0, '#ffffff'),\n",
        "                                                  (0.25, '#000000'),\n",
        "                                                  (1, '#000000')], N=256)\n",
        "  noise_tunnel = NoiseTunnel(integrated_gradients)\n",
        "  target = int(label)\n",
        "  attributions_ig_nt = noise_tunnel.attribute(img, nt_samples=10, nt_type='smoothgrad_sq')\n",
        "  _ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      np.transpose(img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      [\"original_image\", \"heat_map\"],\n",
        "                                      [\"all\", \"positive\"],\n",
        "                                      cmap=default_cmap,\n",
        "                                        show_colorbar=True)\n"
      ],
      "metadata": {
        "id": "WPuYzTrcCgWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pick = 0\n",
        "sample_image = test_split[pick][0]\n",
        "sample_label = test_split[pick][1]\n",
        "visualize_ig(model, sample_image, sample_label, sample_image * 0)"
      ],
      "metadata": {
        "id": "8gWjPBDJao7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 How does occlusion effect our results?\n",
        "\n",
        "Write a function `occlude` that takes in a image and randomly places a grey square that obstructs a part of the image. Then run the code to re-run the classifier on the occluded image comparing the model and integrated gradient output of the occluded verses the non-occluded image. "
      ],
      "metadata": {
        "id": "E_yF692HKhD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def occlude(img):\n",
        "  #Your code goes here\n",
        "  return 0\n",
        "\n",
        "#Code below picks a random image, occludes, and shows the occluded image\n",
        "\n",
        "#Modify pick to run around 5 images to get an idea for the effect\n",
        "pick = 10 #Pick any number between 0 and 100 until you get an idea\n",
        "\n",
        "sample_image = test_split[pick][0]\n",
        "sample_label = test_split[pick][1]\n",
        "sample_image_occluded = occlude(torch.clone(sample_image))\n",
        "fig, ax = plt.subplots(1,1, figsize=(20,20))\n",
        "sample_image_occluded.squeeze(dim=0)\n",
        "\n",
        "\n",
        "ax.title.set_text(\"Metastases\" if sample_label == 1 else \"Benign\")\n",
        "ax.imshow(sample_image_occluded.permute(1,2,0))"
      ],
      "metadata": {
        "id": "VKWO4WjcDhNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Results for the non-occluded image\")\n",
        "visualize_ig(model, sample_image, sample_label, sample_image * 0)\n",
        "\n",
        "print(\"Results for the occluded image\")\n",
        "visualize_ig(model, sample_image_occluded, sample_label, sample_image_occluded * 0)"
      ],
      "metadata": {
        "id": "TA0q7cQLK4Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 How can data augmentation help our model?\n",
        "\n",
        "Our model above might be accurate, but it may or may not have done a good job explaining what metastasis looks like. During the training of our model we rescaled the images as a transformation. There are more transformations that we can do. Fill in `custom_transform` to include a set of other transformations. Then run the code to retrain the model with custom transform and see if your model begins to look for different structures using IG. Refer to the following: https://pytorch.org/docs/stable/torchvision/transforms.html for available transformations. Additionally look at `initial_transform` to see how to compose said transformations."
      ],
      "metadata": {
        "id": "87ytbjg1Qi1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_transform(tensor, label):\n",
        "  \"\"\"\n",
        "  Fill in this function with relevant code. \n",
        "  Think of using transformations like Horizontal Flip or Random Perspective\n",
        "  Remember to first convert the image from np array to PILImage and then resize\n",
        "  to 224 by 224 as this is required by most vision models.\n",
        "  After transformations convert from a PIL image to tensor.\n",
        "  Look at the link provided in the directions for more help.\n",
        " \n",
        "  \"\"\" \n",
        "  return 0\n",
        "\n",
        "pcam = PCam(transform=custom_transform)\n",
        "\n",
        "sz = len(pcam)\n",
        "size_train = math.floor(sz - (sz * (1 - PER_TRAIN)))\n",
        "size_test = sz - size_train\n",
        "\n",
        "print(f\"Training size: {size_train}\")\n",
        "print(f\"Testing size: {size_test}\")\n",
        "\n",
        "train_split, test_split = torch.utils.data.random_split(pcam, [size_train, size_test])\n",
        "train_set = DataLoader(train_split, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, num_workers=1)\n",
        "test_set = DataLoader(test_split, batch_size=1,\n",
        "                        shuffle=True, num_workers=1)\n"
      ],
      "metadata": {
        "id": "e4yWKCXbQlH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training again, read earlier code about how training works \n",
        "model = train(train_set)"
      ],
      "metadata": {
        "id": "GR4LDK9AR56o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving model again but with different name this time\n",
        "model_save_name = 'pcam_model_mod_transforms'\n",
        "path = F\"/content/gdrive/My Drive/BMI702/{model_save_name}\" \n",
        "torch.save(model, path)"
      ],
      "metadata": {
        "id": "AEJQxeSCRFQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading model (code to remount if run into memory issues and have to restart)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_save_name = 'pcam_model_mod_transforms'\n",
        "path = F\"/content/gdrive/My Drive/BMI702/{model_save_name}\" \n",
        "model = torch.load(path)"
      ],
      "metadata": {
        "id": "A1vjd5PgZbYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here you can pick random images, the 0th image is picked right now, to view\n",
        "how IG differs now that you have applied the transform.\n",
        "\n",
        "\"\"\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pick = 0\n",
        "sample_image = test_split[pick][0]\n",
        "sample_label = test_split[pick][1]\n",
        "visualize_ig(model, sample_image, sample_label, sample_image * 0)"
      ],
      "metadata": {
        "id": "j7CaO4ZbWoDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 How does noise effect our model?\n",
        "\n",
        "Fill in the function `noise` to transform an input image by randomly adding noise. Note that the image should almost look identical to the human eye. Run the rest of the code to (1) run the image through the `noise` function and pass it to the classifier, (2) compute its IG, and (3) compare its IG to a non-transformed image. Please note whether or not the performance of your algorithm changes."
      ],
      "metadata": {
        "id": "kV_1Sy7LeHgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noise(tensor):\n",
        "  #Fill in code here to add noise to an input tensor\n",
        "  return 0\n",
        "\n",
        "\"\"\"\n",
        "After filling in the code above, the code below will show the image\n",
        "with added noise in it. It should look like the normal image but with\n",
        "small colored pixel dots everywhere. \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "pick = 0\n",
        "sample_image = test_split[pick][0]\n",
        "sample_label = test_split[pick][1]\n",
        "sample_image_noise = noise(sample_image)\n",
        "sample_image1 = sample_image_noise.unsqueeze(0).to('cpu')\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(20,20))\n",
        "sample_image_noise.squeeze(dim=0)\n",
        "\n",
        "ax.title.set_text(\"Metastases\" if sample_label == 1 else \"Benign\")\n",
        "ax.imshow(sample_image_noise.permute(1,2,0))"
      ],
      "metadata": {
        "id": "RgIEfmXTd8Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Run the code below to compare model output and IG for model with and without \n",
        "noise added. \n",
        "\"\"\"\n",
        "\n",
        "print(\"Results for the regular image\")\n",
        "visualize_ig(model, sample_image, sample_label, sample_image * 0)\n",
        "\n",
        "print(\"Results for the noise-added image\")\n",
        "visualize_ig(model, sample_image_noise, sample_label, sample_image_noise * 0)"
      ],
      "metadata": {
        "id": "E95XRpFDfPsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "46Rer5qLhHQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}